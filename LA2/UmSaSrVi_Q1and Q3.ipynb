{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I have implemented the structure of layers in class and object format of the ones mentioned in the question 1.\n",
    "Then for Question 3, I have loaded the data.\n",
    "Then I have made the layers one after the other and trained the model.\n",
    "Then I have tested the model on same test data, separated in the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MatrixMultiplicationLayer:\n",
    "    def __init__(self, W):\n",
    "        self.W = W\n",
    "\n",
    "    def forward(self, X):\n",
    "        return np.dot(X, self.W)\n",
    "\n",
    "    def backward(self, X, grad_output):\n",
    "        grad_input = np.dot(grad_output, self.W.T)\n",
    "        grad_W = np.dot(X.T, grad_output)\n",
    "        return grad_input, grad_W\n",
    "\n",
    "class BiasAdditionLayer:\n",
    "    def __init__(self, b):\n",
    "        self.b = b\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X + self.b\n",
    "\n",
    "    def backward(self, X, grad_output):\n",
    "        grad_input = grad_output\n",
    "        grad_b = np.sum(grad_output, axis=1)\n",
    "        return grad_input, grad_b\n",
    "    \n",
    "class MeanSquaredLoss:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return 0.5 * np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        return y_pred - y_true\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self, X):\n",
    "        exp_X = np.exp(X)\n",
    "        return exp_X / np.sum(exp_X, axis=1, keepdims=True)\n",
    "\n",
    "    def backward(self, X, grad_output):\n",
    "        softmax = self.forward(X)\n",
    "        return grad_output * (softmax * (1 - softmax))\n",
    "\n",
    "class Sigmoid:\n",
    "    def forward(self, X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "    def backward(self, X, grad_output):\n",
    "        sigmoid = self.forward(X)\n",
    "        return grad_output * sigmoid * (1 - sigmoid)\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        return y_pred - y_true\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# One-hot encode the target labels\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.33, random_state=42)\n",
    "\n",
    "# Define the weight matrix and bias\n",
    "np.random.seed(0)\n",
    "W = np.random.randn(4, 3)\n",
    "b = np.random.randn(1, 3)\n",
    "\n",
    "# Initialize the layers\n",
    "matrix_mult_layer = MatrixMultiplicationLayer(W)\n",
    "bias_add_layer = BiasAdditionLayer(b)\n",
    "softmax_layer = Softmax()\n",
    "cross_entropy_loss = CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "for i in range(1000):\n",
    "    # Forward pass\n",
    "    X_train_out = matrix_mult_layer.forward(X_train)\n",
    "    X_train_out = bias_add_layer.forward(X_train_out)\n",
    "    y_train_pred = softmax_layer.forward(X_train_out)\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = cross_entropy_loss.forward(y_train_pred, y_train)\n",
    "\n",
    "    # Backward pass\n",
    "    grad_output = cross_entropy_loss.backward(y_train_pred, y_train)\n",
    "    grad_input, grad_W = matrix_mult_layer.backward(X_train, grad_output)\n",
    "    grad_input, grad_b = bias_add_layer.backward(X_train_out, grad_input)\n",
    "    \n",
    "    # Update the weight matrix and bias\n",
    "    W -= 0.01 * grad_W\n",
    "    grad_b = np.sum(grad_output, axis=0, keepdims=True)\n",
    "    b -= 0.01 * grad_b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the test data to evaluate the model\n",
    "\n",
    "X_test_out = matrix_mult_layer.forward(X_test)\n",
    "X_test_out = bias_add_layer.forward(X_test_out)\n",
    "y_test_pred = softmax_layer.forward(X_test_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use argmax to select the class with highest probability as the predicted class\n",
    "\n",
    "y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Accuracy score to check the accuracy of the model\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59ddfe56edc95f73912f77c58ed75f9b5f3722acd7cbc9160fd4007e7af55801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I have implemented the structure of layers in class and object format of the ones mentioned in the question 1.\n",
    "Then for Question 2, I have loaded the data and scaled it.\n",
    "Then I have made the layers one after the other and called the training function with 5000 epochs and 0.01 as learning rate.\n",
    "Then I have tested the model on same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MatrixMultiplicationLayer:\n",
    "    def __init__(self, W):\n",
    "        self.W = W\n",
    "\n",
    "    def forward(self, X):\n",
    "        return np.dot(X, self.W)\n",
    "\n",
    "    def backward(self, X, grad_output):\n",
    "        grad_input = np.dot(grad_output, self.W.T)\n",
    "        grad_W = np.dot(X.T, grad_output)\n",
    "        return grad_input, grad_W\n",
    "\n",
    "class BiasAdditionLayer:\n",
    "    def __init__(self, b):\n",
    "        self.b = b\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X + self.b\n",
    "\n",
    "    def backward(self, X, grad_output):\n",
    "        grad_input = grad_output\n",
    "        grad_b = np.sum(grad_output, axis=0)\n",
    "        return grad_input, grad_b\n",
    "    \n",
    "class MeanSquaredLoss:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return 0.5 * np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        return y_pred - y_true\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self, X):\n",
    "        exp_X = np.exp(X)\n",
    "        return exp_X / np.sum(exp_X, axis=1, keepdims=True)\n",
    "\n",
    "    def backward(self, X, grad_output):\n",
    "        softmax = self.forward(X)\n",
    "        return grad_output * (softmax * (1 - softmax))\n",
    "\n",
    "class Sigmoid:\n",
    "    def forward(self, X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "    def backward(self, X, grad_output):\n",
    "        sigmoid = self.forward(X)\n",
    "        return grad_output * sigmoid * (1 - sigmoid)\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        return y_pred - y_true\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onlyu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize layers\n",
    "W = np.random.randn(X.shape[1], 1)\n",
    "b = np.random.randn(1)\n",
    "l1 = MatrixMultiplicationLayer(W)\n",
    "l2 = BiasAdditionLayer(b)\n",
    "loss_layer = MeanSquaredLoss()\n",
    "\n",
    "def train(X, y, W, b, num_epochs, learning_rate):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(X.shape[0]):\n",
    "            # Forward pass\n",
    "            x_i = X[i:i+1] # select ith sample\n",
    "            y_i = y[i:i+1]\n",
    "            z = l1.forward(x_i)\n",
    "            y_pred = l2.forward(z)\n",
    "            loss = loss_layer.forward(y_pred, y_i)\n",
    "\n",
    "            # Backward pass\n",
    "            grad_y_pred = loss_layer.backward(y_pred, y_i)\n",
    "            grad_z, grad_b = l2.backward(z, grad_y_pred)\n",
    "            grad_x, grad_W = l1.backward(x_i, grad_z)\n",
    "\n",
    "            # Update weights\n",
    "            W -= learning_rate * grad_W\n",
    "            b -= learning_rate * grad_b\n",
    "\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "X_test = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# Train the model\n",
    "train(X, y, W, b, num_epochs=5000, learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5403766623188042\n"
     ]
    }
   ],
   "source": [
    "def accuracy(X, y, W, b):\n",
    "    predictions = []\n",
    "    for i in range(X.shape[0]):\n",
    "        x_i = X[i:i+1]\n",
    "        z = l1.forward(x_i)\n",
    "        y_pred = l2.forward(z)\n",
    "        predictions.append(y_pred)\n",
    "    predictions = np.concatenate(predictions)\n",
    "    accuracy = 1 - (np.abs(predictions - y) / y).mean()\n",
    "    return accuracy\n",
    "\n",
    "acc = accuracy(X, y, W, b)\n",
    "print(\"Accuracy:\", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59ddfe56edc95f73912f77c58ed75f9b5f3722acd7cbc9160fd4007e7af55801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
